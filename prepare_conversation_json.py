#!/Users/advait.d/miniconda3/bin/python

import os
import json
import argparse
import random
import re
from functools import partial


'''
Author: Advait Vinay Dhopeshwarkar (advaitvdgoa@gmail.com)
Date  : 25-04-2025

--conversation_text : text file containing conversation generated by an LLM in the required format, and with required tags.
	Check out the prompts section for more details.

--speaker_details : json file containing details of each speaker in the conversation.
	This should be in the format
	{
		"S1":{
			"speakers": [{
				"gender": "xyz",
				"speakerId": "abcd",
				"recorderId": "hijk",
				"nativity": "native-language",
				"ageRange": "40-50"
			}]
		},
		"S2":{
			"speakers": [{
				"gender": "xyz2",
				"speakerId": "abcd2",
				"recorderId": "hijk2",
				"nativity": "native-language2",
				"ageRange": "50-60"
			}]
		}
	}

--conversation_details : json file containing the details of the conversation. The format should be of the following form
	{
		"domain": "Finance",
		"topic": "Investment Planning",
		"language": "bn_IN",
		"conversation_name": "unique_conversation_identifier"
	}

The three inputs are purposely kept separate as these can be generated independently using LLM prompting. Ofcourse the conversation_details
would depend on the conversation, but can be generated keeping the conversation in context of the LLM. Generating these independently works
straightforward. Then use this script to generate the data in the required format.
Observations when trying to generate this data all at once using LLM: The LLM struggles to retain the quality and intricacy of the convers-
ations and the finer details like speech time computation when prompted to generate all at once. Instead, ask the LLM to generate the conv-
ersational content separately in english, then ask the LLM to translate the content to regional language (English conversation -> translat-
ion works slightly better than direct generation as per my observation.)

'''

def get_args():
	parser = argparse.ArgumentParser()
	parser.add_argument("--conversation_text", required=True)
	parser.add_argument("--speaker_details", required=True)
	parser.add_argument("--conversation_details", required=True)
	parser.add_argument("--save_dir", default="output/")
	return parser.parse_args()


def parse_conversation(conversation_text):
	with open(conversation_text, 'r') as f:
		lines = f.read().strip().split('\n')
	speak_lines = []
	for line in lines:
		if line[:2] in ['S1', 'S2']:
			speak_lines.append(line)
		else:
			speak_lines[-1] += line
	lines = speak_lines
	spk2utt = []
	for line in lines:
		line_content = line.split(':')
		spk, content = line_content[0], ' '.join(line_content[1:])
		spk2utt.append((spk, content))
	return spk2utt


def parse_speaker_details(speaker_details):
	with open(speaker_details, 'r') as f:
		spk_det = json.load(f)
	return spk_det


def parse_conversation_details(conversation_details):
	with open(conversation_details, 'r') as f:
		conv_det = json.load(f)
	return conv_det


def _trunc_norm(rng, mean, std, low, high):
	while True:
		x = rng.normalvariate(mean, std)
		if low <= x <= high:
			return x


def estimate_speech_time(sentence):
	'''
	Estimate how long (in seconds) it might take to speak `sentence`.
	The result includes word-dependent timing plus *variable* pauses.
	'''
	rng = random
	words = re.findall(r"[^\s]+", sentence)
	if not words:
		return 0.0

	wpm = max(120, min(200, rng.normalvariate(160, 20)))
	base = len(words) / wpm * 60.0

	pause_specs = {
		",": (0.25, 0.08),
		";": (0.35, 0.10),
		":": (0.35, 0.10),
		".": (0.55, 0.15),
		"?": (0.60, 0.18),
		"!": (0.60, 0.18),
	}

	parts = re.split(r"([,;:.?!])", sentence)
	pauses = 0.0
	for i in range(1, len(parts), 2):
		punct = parts[i]
		prev_clause = parts[i - 1]
		n_clause_words = len(prev_clause.split())

		mean, std = pause_specs[punct]
		# Make longer clauses breathe a little longer (â‰ˆ10 ms / word)
		mean += 0.01 * n_clause_words
		# Draw a *different* random value for **each occurrence**
		pauses += _trunc_norm(rng, mean, std, low=0.05, high=1.5)

	total = (base + pauses) * rng.uniform(0.95, 1.05)
	return round(total, 2)


def main(args):
	conversation_text = args.conversation_text
	speaker_details = args.speaker_details
	conversation_details = args.conversation_details
	save_dir = args.save_dir
	
	os.makedirs(save_dir, exist_ok=True)

	spk2text = parse_conversation(conversation_text)
	spk_det = parse_speaker_details(speaker_details)
	conv_det = parse_conversation_details(conversation_details)
	
	spk2channel = {'S1': 'Left', 'S2': 'Right'}
	spks = ['S1', 'S2']
	spk2file = {}
	for spk in spks:
		spk2file[spk] = os.path.join(save_dir, f"{conv_det['conversation_name']}_{spk2channel[spk]}.json")
	
	speech_duration = {'S1': 0.0, 'S2': 0.0}
	spk2segments = {'S1':[], 'S2': []}
	start_time = round(random.uniform(0, 5), 2)
	for idx, (spk, text) in enumerate(spk2text):
		spk = spk.strip()
		text = text.strip()
		duration = estimate_speech_time(text.strip())
		speech_duration[spk] += duration
		end_time = start_time + duration
		segment = {
			"segmentType": "Speech",
			"segmentId": f"{idx:>05}",
			"start": start_time,
			"end": end_time,
			"transcription": text
		}
		spk2segments[spk].append(segment)
		start_time = end_time + round(random.uniform(0, 3), 2)

	audio_duration = start_time

	# save the output to json files
	for spk in spks:
		output = {
			"domain": conv_det["domain"],
			"topic": conv_det["topic"],
			"language": conv_det["language"],
			"audioDuration": audio_duration,
			"speechDuration": speech_duration[spk],
			"speakers": spk_det[spk]["speakers"],
			"segments": spk2segments[spk]
		}
		with open(spk2file[spk], 'w', encoding='utf-8') as f:
			json.dump(output, f, ensure_ascii=False, indent=2)


if __name__=="__main__":
	args = get_args()
	main(args)
